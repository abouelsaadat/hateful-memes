{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAuZH7U1zRtZ",
        "outputId": "841f2952-9db4-4197-a140-b5dd8a766b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmuBpKaYw7ZG",
        "outputId": "bb75dc0f-a1af-4f89-831a-55d91734cbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/My Drive/Colab Notebooks/cs7643\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/cs7643\")\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vILNx-ewVFRZ",
        "outputId": "1c488d8a-d19d-4645-9547-654694092d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_2020_10_01.tar.gz already exists, skipping download.\n",
            "detectron.lmdb/ already exists, skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# URL of the Hateful Memes features archive\n",
        "URL = \"https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz\"\n",
        "TAR_PATH = \"features_2020_10_01.tar.gz\"\n",
        "EXTRACT_DIR = \"detectron.lmdb\"\n",
        "\n",
        "# 1. Download the tar.gz file (if not already downloaded)\n",
        "if not os.path.exists(TAR_PATH):\n",
        "    print(f\"Downloading from {URL} ...\")\n",
        "    urllib.request.urlretrieve(URL, TAR_PATH)\n",
        "    print(f\"Downloaded to {TAR_PATH}\")\n",
        "else:\n",
        "    print(f\"{TAR_PATH} already exists, skipping download.\")\n",
        "\n",
        "# 2. Extract the tar.gz file (if not already extracted)\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    print(f\"Extracting {TAR_PATH} ...\")\n",
        "    with tarfile.open(TAR_PATH, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "    print(f\"Extraction complete. Files are in: {EXTRACT_DIR}/\")\n",
        "else:\n",
        "    print(f\"{EXTRACT_DIR}/ already exists, skipping extraction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4R0aSgjVfn7",
        "outputId": "aecb998b-0ae9-4aed-9b6a-ee30786fd792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (1.7.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)               # for single GPU\n",
        "    torch.cuda.manual_seed_all(seed)           # if using multi-GPU\n",
        "\n",
        "    # Ensure deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # For some CUDA operations (e.g., DataLoader workers)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "36YTdX7pzQ5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xtnT7Sw3RTzL"
      },
      "outputs": [],
      "source": [
        "import lmdb\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class HatefulMemesDataset(Dataset):\n",
        "    def __init__(self, hf_split, lmdb_path, tokenizer):\n",
        "        \"\"\"\n",
        "        hf_split: one split from the HF DatasetDict (e.g. hf_ds['train'])\n",
        "        lmdb_env: opened lmdb.Environment\n",
        "        tokenizer: HuggingFace tokenizer (optional)\n",
        "        \"\"\"\n",
        "        self.data = hf_split\n",
        "        self.lmdb_path = lmdb_path\n",
        "        self.env = None\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _get_image_id(self, img_path):\n",
        "        # \"img/40259.png\" -> \"40259\"\n",
        "        return img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    def _load_visual_feats(self, img_id):\n",
        "        if self.env is None:  # opened separately in each worker\n",
        "            self.env = lmdb.open(\n",
        "                self.lmdb_path,\n",
        "                readonly=True,\n",
        "                lock=False,\n",
        "                readahead=False,\n",
        "                meminit=False,\n",
        "            )\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            buf = txn.get(img_id.encode(\"utf-8\"))\n",
        "            sample = pickle.loads(buf)\n",
        "        feats = torch.tensor(\n",
        "            sample[\"features\"], dtype=torch.float32\n",
        "        )  # (num_boxes, 2048)\n",
        "        bbox = torch.tensor(sample[\"bbox\"], dtype=torch.float32)  # (num_boxes, 4)\n",
        "        return feats, bbox\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data[idx]\n",
        "        text = row[\"text\"]\n",
        "        img_path = row[\"img\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        img_id = self._get_image_id(img_path)\n",
        "        visual_embeds, _ = self._load_visual_feats(img_id)\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=48,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(0),\n",
        "            \"visual_embeds\": visual_embeds,\n",
        "            \"visual_attention_mask\": torch.ones(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"visual_token_type_ids\": torch.zeros(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX98qNSCRTrv",
        "outputId": "e478f6ee-e308-49a0-a156-78e37ade24df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
        "\n",
        "dataset = load_dataset(\"neuralcatcher/hateful_memes\")\n",
        "# Remove duplicates\n",
        "for i_split, i_data in dataset.items():\n",
        "    dataset[i_split] = Dataset.from_pandas(\n",
        "        pd.DataFrame(i_data).drop_duplicates(), preserve_index=False\n",
        "    )\n",
        "\n",
        "lmdb_path = \"detectron.lmdb\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"train\"], lmdb_path, tokenizer),\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "loader_validation = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"validation\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "loader_test = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"test\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q_ALB6LcS5h2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import VisualBertModel\n",
        "\n",
        "def freeze_all_but_last_n_layers(model, n):\n",
        "    # freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # unfreeze last n encoder layers\n",
        "    total = len(model.encoder.layer)\n",
        "    for i in range(total - n, total):\n",
        "        for p in model.encoder.layer[i].parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # unfreeze pooler (optional)\n",
        "    for p in model.pooler.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "class VisualBertForClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\n",
        "            \"uclanlp/visualbert-vqa-coco-pre\"\n",
        "        )\n",
        "        freeze_all_but_last_n_layers(self.visualbert, n=4)\n",
        "        hidden_size = self.visualbert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        visual_embeds,\n",
        "        visual_attention_mask,\n",
        "        visual_token_type_ids,\n",
        "    ):\n",
        "        outputs = self.visualbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            visual_embeds=visual_embeds,\n",
        "            visual_attention_mask=visual_attention_mask,\n",
        "            visual_token_type_ids=visual_token_type_ids,\n",
        "        )\n",
        "\n",
        "        pooled = outputs.pooler_output  # (B, 768)\n",
        "        logits = self.classifier(self.dropout(pooled))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXCoCoTkV0Cu",
        "outputId": "37599e69-ab65-4eb7-8d0f-605dda253a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "model = VisualBertForClassification().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": model.visualbert.parameters(), \"lr\": 5e-6, \"weight_decay\": 0},\n",
        "    {\"params\": model.classifier.parameters(),  \"lr\": 1e-4, \"weight_decay\": 1e-4},\n",
        "])\n",
        "class_weights = torch.tensor([0.7754, 1.4077], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pi4Bzq8WRTiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e899ce-1418-45be-9ff4-0e9bc831cd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Val Loss: 2.0465 | ACC: 0.6141 | F1: 0.0000 | AUC: 0.5029\n",
            "Epoch 1\n",
            "Train Loss: 95.3630 | ACC: 0.5080 | F1: 0.4098 | AUC: 0.4989\n",
            "Val Loss: 2.0228 | ACC: 0.6266 | F1: 0.1898 | AUC: 0.5599\n",
            "Epoch 2\n",
            "Train Loss: 88.2912 | ACC: 0.6108 | F1: 0.5003 | AUC: 0.6360\n",
            "Val Loss: 1.9747 | ACC: 0.6062 | F1: 0.3298 | AUC: 0.6005\n",
            "Epoch 3\n",
            "Train Loss: 81.9117 | ACC: 0.6736 | F1: 0.5767 | AUC: 0.7200\n",
            "Val Loss: 2.0203 | ACC: 0.6078 | F1: 0.4532 | AUC: 0.6132\n",
            "Epoch 4\n",
            "Train Loss: 78.8574 | ACC: 0.7033 | F1: 0.6022 | AUC: 0.7500\n",
            "Val Loss: 2.0248 | ACC: 0.6109 | F1: 0.4404 | AUC: 0.6222\n",
            "Epoch 5\n",
            "Train Loss: 76.1757 | ACC: 0.7152 | F1: 0.6269 | AUC: 0.7706\n",
            "Val Loss: 2.0263 | ACC: 0.6250 | F1: 0.4393 | AUC: 0.6304\n",
            "Epoch 6\n",
            "Train Loss: 74.0007 | ACC: 0.7301 | F1: 0.6406 | AUC: 0.7871\n",
            "Val Loss: 2.0741 | ACC: 0.6297 | F1: 0.4344 | AUC: 0.6330\n",
            "Epoch 7\n",
            "Train Loss: 72.1780 | ACC: 0.7388 | F1: 0.6553 | AUC: 0.7990\n",
            "Val Loss: 2.0159 | ACC: 0.6375 | F1: 0.4502 | AUC: 0.6425\n",
            "Epoch 8\n",
            "Train Loss: 70.5681 | ACC: 0.7471 | F1: 0.6674 | AUC: 0.8107\n",
            "Val Loss: 2.0263 | ACC: 0.6344 | F1: 0.4658 | AUC: 0.6426\n",
            "Epoch 9\n",
            "Train Loss: 68.0494 | ACC: 0.7572 | F1: 0.6775 | AUC: 0.8257\n",
            "Val Loss: 2.0681 | ACC: 0.6469 | F1: 0.4593 | AUC: 0.6466\n",
            "Epoch 10\n",
            "Train Loss: 64.1940 | ACC: 0.7776 | F1: 0.7061 | AUC: 0.8480\n",
            "Val Loss: 2.1351 | ACC: 0.6391 | F1: 0.4809 | AUC: 0.6519\n",
            "Epoch 11\n",
            "Train Loss: 62.2537 | ACC: 0.7866 | F1: 0.7177 | AUC: 0.8592\n",
            "Val Loss: 2.0698 | ACC: 0.6359 | F1: 0.5074 | AUC: 0.6568\n",
            "Epoch 12\n",
            "Train Loss: 61.2398 | ACC: 0.7906 | F1: 0.7221 | AUC: 0.8639\n",
            "Val Loss: 2.0460 | ACC: 0.6453 | F1: 0.5054 | AUC: 0.6671\n",
            "Epoch 13\n",
            "Train Loss: 59.6742 | ACC: 0.7964 | F1: 0.7302 | AUC: 0.8706\n",
            "Val Loss: 2.1146 | ACC: 0.6312 | F1: 0.4537 | AUC: 0.6601\n",
            "Epoch 14\n",
            "Train Loss: 56.6402 | ACC: 0.8147 | F1: 0.7519 | AUC: 0.8852\n",
            "Val Loss: 2.1702 | ACC: 0.6406 | F1: 0.5363 | AUC: 0.6684\n",
            "Epoch 15\n",
            "Train Loss: 55.8770 | ACC: 0.8119 | F1: 0.7496 | AUC: 0.8879\n",
            "Val Loss: 2.2322 | ACC: 0.6391 | F1: 0.4665 | AUC: 0.6630\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "keys2pass = [\n",
        "    \"input_ids\",\n",
        "    \"attention_mask\",\n",
        "    \"token_type_ids\",\n",
        "    \"visual_embeds\",\n",
        "    \"visual_attention_mask\",\n",
        "    \"visual_token_type_ids\",\n",
        "]\n",
        "model.eval()\n",
        "all_preds_validation = []\n",
        "all_probs_validation = []\n",
        "all_labels_validation = []\n",
        "val_loss = 0\n",
        "with torch.inference_mode():\n",
        "    for batch in loader_validation:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs_validation.extend(probs)\n",
        "        all_preds_validation.extend(preds)\n",
        "        all_labels_validation.extend(labels)\n",
        "\n",
        "acc = accuracy_score(all_labels_validation, all_preds_validation)\n",
        "f1 = f1_score(all_labels_validation, all_preds_validation)\n",
        "auc = roc_auc_score(all_labels_validation, all_probs_validation)\n",
        "print(\"Epoch 0\")\n",
        "print(f\"Val Loss: {val_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    all_preds_train = []\n",
        "    all_probs_train = []\n",
        "    all_labels_train = []\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    for batch in loader_train:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].detach().cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs_train.extend(probs)\n",
        "        all_preds_train.extend(preds)\n",
        "        all_labels_train.extend(labels)\n",
        "    acc = accuracy_score(all_labels_train, all_preds_train)\n",
        "    f1 = f1_score(all_labels_train, all_preds_train)\n",
        "    auc = roc_auc_score(all_labels_train, all_probs_train)\n",
        "    print(f\"Train Loss: {total_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    all_preds_validation = []\n",
        "    all_probs_validation = []\n",
        "    all_labels_validation = []\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch in loader_validation:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**{k: batch[k] for k in keys2pass})\n",
        "            loss = criterion(logits, batch[\"label\"])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_probs_validation.extend(probs)\n",
        "            all_preds_validation.extend(preds)\n",
        "            all_labels_validation.extend(labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels_validation, all_preds_validation)\n",
        "    f1 = f1_score(all_labels_validation, all_preds_validation)\n",
        "    auc = roc_auc_score(all_labels_validation, all_probs_validation)\n",
        "    print(f\"Val Loss: {val_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EzKboprh_tHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f884feb-07e5-424f-e41f-4f34fa8f64cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 9.210083335638046\n",
            "Test Acc: 0.6433333333333333\n",
            "Test F1: 0.5201793721973094\n",
            "Test AUC: 0.6763407258064517\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "test_loss = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader_test:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs.extend(probs)\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Acc:\", acc)\n",
        "print(\"Test F1:\", f1)\n",
        "print(\"Test AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}