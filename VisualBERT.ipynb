{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAuZH7U1zRtZ",
        "outputId": "f69429d7-80d9-4865-ef36-ad9679f58e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmuBpKaYw7ZG",
        "outputId": "b2edfa50-c79e-4541-f0e9-6e293416dbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/My Drive/Colab Notebooks/cs7643\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/cs7643\")\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vILNx-ewVFRZ",
        "outputId": "d49cb5c3-1cdc-46a2-f446-4b999f625a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_2020_10_01.tar.gz already exists, skipping download.\n",
            "detectron.lmdb/ already exists, skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# URL of the Hateful Memes features archive\n",
        "URL = \"https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz\"\n",
        "TAR_PATH = \"features_2020_10_01.tar.gz\"\n",
        "EXTRACT_DIR = \"detectron.lmdb\"\n",
        "\n",
        "# 1. Download the tar.gz file (if not already downloaded)\n",
        "if not os.path.exists(TAR_PATH):\n",
        "    print(f\"Downloading from {URL} ...\")\n",
        "    urllib.request.urlretrieve(URL, TAR_PATH)\n",
        "    print(f\"Downloaded to {TAR_PATH}\")\n",
        "else:\n",
        "    print(f\"{TAR_PATH} already exists, skipping download.\")\n",
        "\n",
        "# 2. Extract the tar.gz file (if not already extracted)\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    print(f\"Extracting {TAR_PATH} ...\")\n",
        "    with tarfile.open(TAR_PATH, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "    print(f\"Extraction complete. Files are in: {EXTRACT_DIR}/\")\n",
        "else:\n",
        "    print(f\"{EXTRACT_DIR}/ already exists, skipping extraction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4R0aSgjVfn7",
        "outputId": "8d0027d9-f9fc-48f6-a773-cbc7c92c30cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (1.7.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)               # for single GPU\n",
        "    torch.cuda.manual_seed_all(seed)           # if using multi-GPU\n",
        "\n",
        "    # Ensure deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # For some CUDA operations (e.g., DataLoader workers)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "36YTdX7pzQ5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xtnT7Sw3RTzL"
      },
      "outputs": [],
      "source": [
        "import lmdb\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class HatefulMemesDataset(Dataset):\n",
        "    def __init__(self, hf_split, lmdb_path, tokenizer):\n",
        "        \"\"\"\n",
        "        hf_split: one split from the HF DatasetDict (e.g. hf_ds['train'])\n",
        "        lmdb_env: opened lmdb.Environment\n",
        "        tokenizer: HuggingFace tokenizer (optional)\n",
        "        \"\"\"\n",
        "        self.data = hf_split\n",
        "        self.lmdb_path = lmdb_path\n",
        "        self.env = None\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _get_image_id(self, img_path):\n",
        "        # \"img/40259.png\" -> \"40259\"\n",
        "        return img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    def _load_visual_feats(self, img_id):\n",
        "        if self.env is None:  # opened separately in each worker\n",
        "            self.env = lmdb.open(\n",
        "                self.lmdb_path,\n",
        "                readonly=True,\n",
        "                lock=False,\n",
        "                readahead=False,\n",
        "                meminit=False,\n",
        "            )\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            buf = txn.get(img_id.encode(\"utf-8\"))\n",
        "            sample = pickle.loads(buf)\n",
        "        feats = torch.tensor(\n",
        "            sample[\"features\"], dtype=torch.float32\n",
        "        )  # (num_boxes, 2048)\n",
        "        bbox = torch.tensor(sample[\"bbox\"], dtype=torch.float32)  # (num_boxes, 4)\n",
        "        return feats, bbox\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data[idx]\n",
        "        text = row[\"text\"]\n",
        "        img_path = row[\"img\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        img_id = self._get_image_id(img_path)\n",
        "        visual_embeds, _ = self._load_visual_feats(img_id)\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=48,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(0),\n",
        "            \"visual_embeds\": visual_embeds,\n",
        "            \"visual_attention_mask\": torch.ones(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"visual_token_type_ids\": torch.zeros(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu8X6v7WsUW5",
        "outputId": "866371d3-a070-46bc-b03d-6e5d28fe45c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX98qNSCRTrv",
        "outputId": "7f87e304-0b12-49e2-8233-01f6d3af87b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
        "\n",
        "dataset = load_dataset(\"neuralcatcher/hateful_memes\")\n",
        "# Remove duplicates\n",
        "for i_split, i_data in dataset.items():\n",
        "    dataset[i_split] = Dataset.from_pandas(\n",
        "        pd.DataFrame(i_data).drop_duplicates(), preserve_index=False\n",
        "    )\n",
        "\n",
        "lmdb_path = \"detectron.lmdb\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"train\"], lmdb_path, tokenizer),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "loader_validation = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"validation\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "loader_test = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"test\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q_ALB6LcS5h2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import VisualBertModel\n",
        "\n",
        "def freeze_all_but_last_n_layers(model, n):\n",
        "    # freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # unfreeze last n encoder layers\n",
        "    total = len(model.encoder.layer)\n",
        "    for i in range(total - n, total):\n",
        "        for p in model.encoder.layer[i].parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # unfreeze pooler (optional)\n",
        "    for p in model.pooler.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "class VisualBertForClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\n",
        "            \"uclanlp/visualbert-vqa-coco-pre\"\n",
        "        )\n",
        "        freeze_all_but_last_n_layers(self.visualbert, n=9)\n",
        "        hidden_size = self.visualbert.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        visual_embeds,\n",
        "        visual_attention_mask,\n",
        "        visual_token_type_ids,\n",
        "    ):\n",
        "        outputs = self.visualbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            visual_embeds=visual_embeds,\n",
        "            visual_attention_mask=visual_attention_mask,\n",
        "            visual_token_type_ids=visual_token_type_ids,\n",
        "        )\n",
        "\n",
        "        pooled = outputs.pooler_output  # (B, 768)\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXCoCoTkV0Cu",
        "outputId": "fc5a8532-de93-47b2-cb68-c3007c73c853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "model = VisualBertForClassification().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": model.visualbert.parameters(), \"lr\": 2e-5, \"weight_decay\": 0},\n",
        "    {\"params\": model.classifier.parameters(),  \"lr\": 2e-3, \"weight_decay\": 1e-5},\n",
        "])\n",
        "class_weights = torch.tensor([0.7754, 1.4077], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.00)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_auc = 0.0\n",
        "best_model_path = \"best_visualbert.pt\""
      ],
      "metadata": {
        "id": "1GATnKkCOazf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pi4Bzq8WRTiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a76e34b-748f-41d5-d031-468bf27a07ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Val Loss: 2.0465 | ACC: 0.6141 | F1: 0.0000 | AUC: 0.5029\n",
            "Epoch 1\n",
            "Train Loss: 169.5963 | ACC: 0.6400 | F1: 0.5358 | AUC: 0.6860\n",
            "Val Loss: 2.0035 | ACC: 0.6109 | F1: 0.3882 | AUC: 0.6360\n",
            "Epoch 2\n",
            "Train Loss: 137.9868 | ACC: 0.7588 | F1: 0.6718 | AUC: 0.8182\n",
            "Val Loss: 2.0039 | ACC: 0.6297 | F1: 0.4925 | AUC: 0.6661\n",
            "Epoch 3\n",
            "Train Loss: 113.7144 | ACC: 0.8075 | F1: 0.7408 | AUC: 0.8842\n",
            "Val Loss: 2.0152 | ACC: 0.6531 | F1: 0.5316 | AUC: 0.6868\n",
            "Epoch 4\n",
            "Train Loss: 89.0332 | ACC: 0.8562 | F1: 0.8089 | AUC: 0.9314\n",
            "Val Loss: 2.6209 | ACC: 0.6641 | F1: 0.5316 | AUC: 0.6831\n",
            "Epoch 5\n",
            "Train Loss: 70.1693 | ACC: 0.8845 | F1: 0.8462 | AUC: 0.9571\n",
            "Val Loss: 2.9701 | ACC: 0.6656 | F1: 0.4541 | AUC: 0.6900\n",
            "Epoch 6\n",
            "Train Loss: 55.6254 | ACC: 0.9118 | F1: 0.8818 | AUC: 0.9730\n",
            "Val Loss: 2.5857 | ACC: 0.6766 | F1: 0.5470 | AUC: 0.6884\n",
            "Epoch 7\n",
            "Train Loss: 42.5944 | ACC: 0.9333 | F1: 0.9098 | AUC: 0.9837\n",
            "Val Loss: 3.0347 | ACC: 0.6609 | F1: 0.4642 | AUC: 0.6903\n",
            "Epoch 8\n",
            "Train Loss: 35.2333 | ACC: 0.9445 | F1: 0.9246 | AUC: 0.9885\n",
            "Val Loss: 3.9442 | ACC: 0.6625 | F1: 0.5242 | AUC: 0.6903\n",
            "Epoch 9\n",
            "Train Loss: 29.0847 | ACC: 0.9556 | F1: 0.9393 | AUC: 0.9924\n",
            "Val Loss: 4.6437 | ACC: 0.6484 | F1: 0.4731 | AUC: 0.6785\n",
            "Epoch 10\n",
            "Train Loss: 21.9232 | ACC: 0.9678 | F1: 0.9555 | AUC: 0.9954\n",
            "Val Loss: 4.3355 | ACC: 0.6547 | F1: 0.5056 | AUC: 0.6883\n",
            "Epoch 11\n",
            "Train Loss: 23.8712 | ACC: 0.9647 | F1: 0.9514 | AUC: 0.9948\n",
            "Val Loss: 4.9259 | ACC: 0.6641 | F1: 0.4327 | AUC: 0.6967\n",
            "Epoch 12\n",
            "Train Loss: 17.8204 | ACC: 0.9764 | F1: 0.9671 | AUC: 0.9966\n",
            "Val Loss: 4.7512 | ACC: 0.6687 | F1: 0.4976 | AUC: 0.6923\n",
            "Epoch 13\n",
            "Train Loss: 16.1294 | ACC: 0.9794 | F1: 0.9713 | AUC: 0.9972\n",
            "Val Loss: 4.4559 | ACC: 0.6594 | F1: 0.3807 | AUC: 0.6937\n",
            "Epoch 14\n",
            "Train Loss: 13.7081 | ACC: 0.9807 | F1: 0.9731 | AUC: 0.9981\n",
            "Val Loss: 5.2139 | ACC: 0.6641 | F1: 0.4769 | AUC: 0.6987\n",
            "Epoch 15\n",
            "Train Loss: 13.5483 | ACC: 0.9824 | F1: 0.9753 | AUC: 0.9983\n",
            "Val Loss: 5.1359 | ACC: 0.6656 | F1: 0.4953 | AUC: 0.7045\n",
            "Epoch 16\n",
            "Train Loss: 10.2582 | ACC: 0.9874 | F1: 0.9824 | AUC: 0.9988\n",
            "Val Loss: 6.4723 | ACC: 0.6625 | F1: 0.4066 | AUC: 0.6882\n",
            "Epoch 17\n",
            "Train Loss: 11.2688 | ACC: 0.9875 | F1: 0.9826 | AUC: 0.9985\n",
            "Val Loss: 5.0066 | ACC: 0.6734 | F1: 0.4788 | AUC: 0.6917\n",
            "Epoch 18\n",
            "Train Loss: 8.6566 | ACC: 0.9887 | F1: 0.9842 | AUC: 0.9991\n",
            "Val Loss: 6.2969 | ACC: 0.6438 | F1: 0.4062 | AUC: 0.6754\n",
            "Epoch 19\n",
            "Train Loss: 8.1531 | ACC: 0.9895 | F1: 0.9853 | AUC: 0.9991\n",
            "Val Loss: 7.3687 | ACC: 0.6547 | F1: 0.4377 | AUC: 0.6726\n",
            "Epoch 20\n",
            "Train Loss: 8.8460 | ACC: 0.9902 | F1: 0.9863 | AUC: 0.9989\n",
            "Val Loss: 6.4510 | ACC: 0.6875 | F1: 0.5215 | AUC: 0.7077\n",
            "--> Saved best model (AUC=0.7077)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "keys2pass = [\n",
        "    \"input_ids\",\n",
        "    \"attention_mask\",\n",
        "    \"token_type_ids\",\n",
        "    \"visual_embeds\",\n",
        "    \"visual_attention_mask\",\n",
        "    \"visual_token_type_ids\",\n",
        "]\n",
        "model.eval()\n",
        "all_preds_validation = []\n",
        "all_probs_validation = []\n",
        "all_labels_validation = []\n",
        "val_loss = 0\n",
        "with torch.inference_mode():\n",
        "    for batch in loader_validation:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs_validation.extend(probs)\n",
        "        all_preds_validation.extend(preds)\n",
        "        all_labels_validation.extend(labels)\n",
        "\n",
        "acc = accuracy_score(all_labels_validation, all_preds_validation)\n",
        "f1 = f1_score(all_labels_validation, all_preds_validation)\n",
        "auc = roc_auc_score(all_labels_validation, all_probs_validation)\n",
        "print(\"Epoch 0\")\n",
        "print(f\"Val Loss: {val_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    all_preds_train = []\n",
        "    all_probs_train = []\n",
        "    all_labels_train = []\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    for batch in loader_train:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        loss.backward()\n",
        "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].detach().cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs_train.extend(probs)\n",
        "        all_preds_train.extend(preds)\n",
        "        all_labels_train.extend(labels)\n",
        "    acc = accuracy_score(all_labels_train, all_preds_train)\n",
        "    f1 = f1_score(all_labels_train, all_preds_train)\n",
        "    auc = roc_auc_score(all_labels_train, all_probs_train)\n",
        "    print(f\"Train Loss: {total_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    all_preds_validation = []\n",
        "    all_probs_validation = []\n",
        "    all_labels_validation = []\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch in loader_validation:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**{k: batch[k] for k in keys2pass})\n",
        "            loss = criterion(logits, batch[\"label\"])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_probs_validation.extend(probs)\n",
        "            all_preds_validation.extend(preds)\n",
        "            all_labels_validation.extend(labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels_validation, all_preds_validation)\n",
        "    f1 = f1_score(all_labels_validation, all_preds_validation)\n",
        "    auc = roc_auc_score(all_labels_validation, all_probs_validation)\n",
        "    print(f\"Val Loss: {val_loss:.4f} | ACC: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "    # Save best model based on AUC\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"--> Saved best model (AUC={best_auc:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EzKboprh_tHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566927fc-d5da-4c84-edc6-9698a0786829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 26.971502661705017\n",
            "Test Acc: 0.682\n",
            "Test F1: 0.5364431486880467\n",
            "Test AUC: 0.719469620601173\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "model = VisualBertForClassification().to(device)\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "test_loss = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader_test:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs.extend(probs)\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Acc:\", acc)\n",
        "print(\"Test F1:\", f1)\n",
        "print(\"Test AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}