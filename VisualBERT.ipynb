{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAuZH7U1zRtZ",
        "outputId": "667ee9b5-d751-42a1-959a-a44fbe040cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmuBpKaYw7ZG",
        "outputId": "baf11029-eabe-4d62-89b9-4c45b989bfb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/My Drive/Colab Notebooks/cs7643\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/cs7643\")\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vILNx-ewVFRZ",
        "outputId": "2a49cddc-88c9-4c22-8cc3-648482cf6a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_2020_10_01.tar.gz already exists, skipping download.\n",
            "detectron.lmdb/ already exists, skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# URL of the Hateful Memes features archive\n",
        "URL = \"https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz\"\n",
        "TAR_PATH = \"features_2020_10_01.tar.gz\"\n",
        "EXTRACT_DIR = \"detectron.lmdb\"\n",
        "\n",
        "# 1. Download the tar.gz file (if not already downloaded)\n",
        "if not os.path.exists(TAR_PATH):\n",
        "    print(f\"Downloading from {URL} ...\")\n",
        "    urllib.request.urlretrieve(URL, TAR_PATH)\n",
        "    print(f\"Downloaded to {TAR_PATH}\")\n",
        "else:\n",
        "    print(f\"{TAR_PATH} already exists, skipping download.\")\n",
        "\n",
        "# 2. Extract the tar.gz file (if not already extracted)\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    print(f\"Extracting {TAR_PATH} ...\")\n",
        "    with tarfile.open(TAR_PATH, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "    print(f\"Extraction complete. Files are in: {EXTRACT_DIR}/\")\n",
        "else:\n",
        "    print(f\"{EXTRACT_DIR}/ already exists, skipping extraction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4R0aSgjVfn7",
        "outputId": "a066733b-906e-4a00-9d1f-0e4b86d19017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (1.7.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xtnT7Sw3RTzL"
      },
      "outputs": [],
      "source": [
        "import lmdb\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class HatefulMemesDataset(Dataset):\n",
        "    def __init__(self, hf_split, lmdb_path, tokenizer):\n",
        "        \"\"\"\n",
        "        hf_split: one split from the HF DatasetDict (e.g. hf_ds['train'])\n",
        "        lmdb_env: opened lmdb.Environment\n",
        "        tokenizer: HuggingFace tokenizer (optional)\n",
        "        \"\"\"\n",
        "        self.data = hf_split\n",
        "        self.lmdb_path = lmdb_path\n",
        "        self.env = None\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _get_image_id(self, img_path):\n",
        "        # \"img/40259.png\" -> \"40259\"\n",
        "        return img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    def _load_visual_feats(self, img_id):\n",
        "        if self.env is None:  # opened separately in each worker\n",
        "            self.env = lmdb.open(\n",
        "                self.lmdb_path,\n",
        "                readonly=True,\n",
        "                lock=False,\n",
        "                readahead=False,\n",
        "                meminit=False,\n",
        "            )\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            buf = txn.get(img_id.encode(\"utf-8\"))\n",
        "            sample = pickle.loads(buf)\n",
        "        feats = torch.tensor(\n",
        "            sample[\"features\"], dtype=torch.float32\n",
        "        )  # (num_boxes, 2048)\n",
        "        bbox = torch.tensor(sample[\"bbox\"], dtype=torch.float32)  # (num_boxes, 4)\n",
        "        return feats, bbox\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data[idx]\n",
        "        text = row[\"text\"]\n",
        "        img_path = row[\"img\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        img_id = self._get_image_id(img_path)\n",
        "        visual_embeds, _ = self._load_visual_feats(img_id)\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=48,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"token_type_ids\": encoded[\"token_type_ids\"].squeeze(0),\n",
        "            \"visual_embeds\": visual_embeds,\n",
        "            \"visual_attention_mask\": torch.ones(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"visual_token_type_ids\": torch.zeros(\n",
        "                visual_embeds.size(0), dtype=torch.long\n",
        "            ),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX98qNSCRTrv",
        "outputId": "d3d0dab7-7948-4119-fb03-6cb1e57a9a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
        "\n",
        "lmdb_path = \"detectron.lmdb\"\n",
        "dataset = load_dataset(\"neuralcatcher/hateful_memes\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"train\"], lmdb_path, tokenizer),\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=16,\n",
        ")\n",
        "\n",
        "loader_validation = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"validation\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=16,\n",
        ")\n",
        "\n",
        "loader_test = DataLoader(\n",
        "    HatefulMemesDataset(dataset[\"test\"], lmdb_path, tokenizer),\n",
        "    batch_size=265,\n",
        "    shuffle=False,\n",
        "    num_workers=16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "q_ALB6LcS5h2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import VisualBertModel\n",
        "\n",
        "\n",
        "class VisualBertForClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\n",
        "            \"uclanlp/visualbert-vqa-coco-pre\"\n",
        "        )\n",
        "        hidden_size = self.visualbert.config.hidden_size  # usually 768\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        visual_embeds,\n",
        "        visual_attention_mask,\n",
        "        visual_token_type_ids,\n",
        "    ):\n",
        "        outputs = self.visualbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            visual_embeds=visual_embeds,\n",
        "            visual_attention_mask=visual_attention_mask,\n",
        "            visual_token_type_ids=visual_token_type_ids,\n",
        "        )\n",
        "\n",
        "        pooled = outputs.pooler_output  # (B, 768)\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXCoCoTkV0Cu",
        "outputId": "8d530605-d176-4181-d25c-0e1e2d7cd885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "model = VisualBertForClassification().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "pi4Bzq8WRTiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffca6aea-c240-4259-998f-a91365cfd525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 41.1347 | Val Loss: 2.8829\n",
            "Epoch 2 | Train Loss: 33.2356 | Val Loss: 3.1917\n",
            "Epoch 3 | Train Loss: 26.9544 | Val Loss: 3.5634\n",
            "Epoch 4 | Train Loss: 22.4512 | Val Loss: 3.3632\n",
            "Epoch 5 | Train Loss: 17.9093 | Val Loss: 3.8379\n",
            "Epoch 6 | Train Loss: 14.8730 | Val Loss: 4.5083\n",
            "Epoch 7 | Train Loss: 12.5656 | Val Loss: 4.6028\n",
            "Epoch 8 | Train Loss: 9.3023 | Val Loss: 4.5283\n",
            "Epoch 9 | Train Loss: 7.5412 | Val Loss: 6.1034\n",
            "Epoch 10 | Train Loss: 6.7044 | Val Loss: 5.5882\n"
          ]
        }
      ],
      "source": [
        "keys2pass = [\n",
        "    \"input_ids\",\n",
        "    \"attention_mask\",\n",
        "    \"token_type_ids\",\n",
        "    \"visual_embeds\",\n",
        "    \"visual_attention_mask\",\n",
        "    \"visual_token_type_ids\",\n",
        "]\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch+1} | \",  end='')\n",
        "    for batch in loader_train:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train Loss: {total_loss:.4f} | \",  end='')\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch in loader_validation:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**{k: batch[k] for k in keys2pass})\n",
        "            loss = criterion(logits, batch[\"label\"])\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzKboprh_tHR",
        "outputId": "8ca26ff3-172f-4d0a-c847-d40c7423686d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 14.268611058592796\n",
            "Test Acc: 0.6753333333333333\n",
            "Test F1: 0.508577194752775\n",
            "Test AUC: 0.7156644061583577\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "test_loss = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader_test:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = model(**{k: batch[k] for k in keys2pass})\n",
        "        loss = criterion(logits, batch[\"label\"])\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "        all_probs.extend(probs)\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Acc:\", acc)\n",
        "print(\"Test F1:\", f1)\n",
        "print(\"Test AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}